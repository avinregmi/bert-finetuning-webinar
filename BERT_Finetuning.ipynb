{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-Finetuning-Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bTU_UPlfRor",
        "colab_type": "text"
      },
      "source": [
        "# BERT Finetuning with Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiEyOBHLfk5d",
        "colab_type": "text"
      },
      "source": [
        "## Understanding the Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZcTLiMrXC_Q",
        "colab_type": "code",
        "outputId": "c476ef2f-867f-49a5-d984-c54fc82d1d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 3.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 71.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=54f6d8b9a27f2adc42117c3cb3f02c32190b5f2e989757ddaa58a0e37b295ae9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozgNVDWgYinS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "outputId": "7bb9bc7c-58fe-47d6-ad1e-1d7178f3e90a"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_b_zYKHZAMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2_BQ3A2gHJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'he likes to play'\n",
        "# Step 1: Tokenize\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "# Step 2: Add [CLS] and [SEP]\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "# Step 3: Pad tokens\n",
        "padded_tokens = tokens + ['[PAD]' for _ in range(20 - len(tokens))]\n",
        "attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
        "# Step 4: Segment ids\n",
        "seg_ids = [0 for _ in range(len(padded_tokens))] #Optional!\n",
        "# Step 5: Get BERT vocabulary index for each token\n",
        "token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwwjM45ygHs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to pytorch tensors\n",
        "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
        "attn_mask = torch.tensor(attn_mask).unsqueeze(0)\n",
        "seg_ids = torch.tensor(seg_ids).unsqueeze(0)\n",
        "\n",
        "# Feed them to bert\n",
        "hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask,\\\n",
        "                                  token_type_ids = seg_ids)\n",
        "print(hidden_reps.shape)\n",
        "print(cls_head.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CTYNI09qN4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking that [CLS] representation from hidden_repr is not equal to [CLS] representation from cls_head\n",
        "torch.all(hidden_reps[0][0].eq(cls_head[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4uMPQ_Km9_E",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Class and Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDF8EGbJSAmT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "6b45399b-2f60-45ca-f8b6-0522b58c0533"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=afe72cb3ceea1208fb54eb190499b1b402871a280d49857b34d91d171cff0ffd\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GtmTUYSSCHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4443edd-c296-4378-d98b-18d174786365"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://raw.githubusercontent.com/theneuralbeing/bert-finetuning-webinar/master/data.zip'\n",
        "\n",
        "# Download the file and unzip it (if we haven't already)\n",
        "if not os.path.exists('./data.zip'):\n",
        "    wget.download(url, './data.zip')\n",
        "    !unzip -q data.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwmzHMnzn6ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsxKO4u-iTOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LoadDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen):\n",
        "\n",
        "        # Store the contents of the file in a pandas dataframe\n",
        "        self.df = pd.read_csv(filename, delimiter=',')\n",
        "\n",
        "        # Initialize the BERT tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Define the Maxlength for padding/truncating\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, 'review']\n",
        "        label = self.df.loc[index, 'sentiment']\n",
        "\n",
        "        # Tokenize the sentence\n",
        "        tokens = self.tokenizer.tokenize(sentence)\n",
        "\n",
        "        # Inserting the CLS and SEP token at the beginning and end of the sentence\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        \n",
        "        # Padding/truncating the sentences to the maximum length\n",
        "        if len(tokens) < self.maxlen:\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]\n",
        "        else:\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]']\n",
        "        \n",
        "        # Convert the sequence to ids with BERT Vocabulary\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        \n",
        "        # Converting the list to a pytorch tensor\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "\n",
        "        # Obtaining the attention mask\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "\n",
        "        return tokens_ids_tensor, attn_mask, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqe1Tn_gU-mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating instances of training and validation set\n",
        "train_set = LoadDataset(filename = 'train.csv', maxlen = 64)\n",
        "val_set = LoadDataset(filename = 'validation.csv', maxlen = 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQBez4GyVISF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating intsances of training and validation dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size = 32, num_workers = 5)\n",
        "val_loader = DataLoader(val_set, batch_size = 32, num_workers = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy59x1OzMnne",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q5zrt4MonMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyKJ_lHWTi_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, freeze_bert = True):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "\n",
        "        # Instantiating the BERT model object \n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        # Defining layers like dropout and linear\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        # Getting contextualized representations from BERT Layer\n",
        "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "\n",
        "        # Obtaining the representation of [CLS] head\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        # Feeding cls_rep to the classifier layer\n",
        "        logits = self.classifier(cls_rep)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vifcpKtlU8y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SentimentClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEFyqz_MMvjJ",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOZ_RBDOU68m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "criterion = BCEWithLogitsLoss()\n",
        "optimizer = Adam(model.parameters(), lr = 2e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrFxN3mLwLMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2438746-9d4e-4458-a81f-ab583f99d1ad"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gubu6At42C_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function for calculating accuracy\n",
        "def logits_accuracy(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    preds = (probs > 0.5).long()\n",
        "    acc = (preds.squeeze() == labels).float().mean()\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPYi4_6_2C2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining an evaluation function for training \n",
        "def evaluate(net, criterion, val_loader, device):\n",
        "  \n",
        "    losses, accuracies = 0, 0\n",
        "    \n",
        "    # Setting model to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    count = 0\n",
        "    for (seq, attn_masks, labels) in val_loader:\n",
        "        count += 1\n",
        "\n",
        "        # Move inputs and targets to device\n",
        "        seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
        "\n",
        "        # Get logit predictions\n",
        "        val_logits = net(seq, attn_masks)\n",
        "\n",
        "        # Calculate loss\n",
        "        val_loss = criterion(val_logits.squeeze(-1), labels.float())\n",
        "        losses += val_loss.item()\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        accuracies += logits_accuracy(val_logits, labels)\n",
        "\n",
        "    return losses / count, accuracies / count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OvRi_i4BYqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-95E-ZVVNHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, criterion, optimizer, train_loader, val_loader, device, epochs=4, print_every=100):\n",
        "    \n",
        "    # Move model to device\n",
        "    net.to(device)\n",
        "    # Setting model to training mode\n",
        "    net.train()\n",
        "\n",
        "    print('========== ========== STARTING TRAINING ========== ==========')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print('\\n\\n========== EPOCH {} =========='.format(epoch))        \n",
        "        t1 = time()\n",
        "\n",
        "        for i, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()  \n",
        "\n",
        "            # Moving tensors to device\n",
        "            seq, attn_masks, labels = seq.to(device), attn_masks.to(device), labels.to(device)\n",
        "\n",
        "            # Obtaining the logits from the model\n",
        "            logits = net(seq,attn_masks)\n",
        "\n",
        "            # Calculating the loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clipping gradients to tackle exploding gradients\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "\n",
        "            # Optimization step\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(\"Iteration {} ==== Loss: {}\".format(i+1, loss.item()))\n",
        "\n",
        "        t2 = time()\n",
        "        print('Time Taken for Epoch: {}'.format(t2-t1))\n",
        "        print('\\n========== Validating ==========')\n",
        "        mean_val_loss, mean_val_acc = evaluate(net, criterion, val_loader, device)\n",
        "        print(\"Validation Loss: {}\\nValidation Accuracy: {}\".format(mean_val_loss, mean_val_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWAQPhpoVd2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "204151c3-37ea-4c86-cdd7-edced1f41a7a"
      },
      "source": [
        "%%time\n",
        "# starting training\n",
        "train(model, criterion, optimizer, train_loader, val_loader, device, print_every=100)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== ========== STARTING TRAINING ========== ==========\n",
            "\n",
            "\n",
            "========== EPOCH 0 ==========\n",
            "Iteration 100 ==== Loss: 0.4057803750038147\n",
            "Iteration 200 ==== Loss: 0.6811727285385132\n",
            "Iteration 300 ==== Loss: 0.39617377519607544\n",
            "Iteration 400 ==== Loss: 0.3085844814777374\n",
            "Iteration 500 ==== Loss: 0.3079783320426941\n",
            "Iteration 600 ==== Loss: 0.3183799684047699\n",
            "Iteration 700 ==== Loss: 0.4045353829860687\n",
            "Time Taken for Epoch: 212.97628712654114\n",
            "\n",
            "========== Validating ==========\n",
            "Validation Loss: 0.37708764637598907\n",
            "Validation Accuracy: 0.8322410583496094\n",
            "\n",
            "\n",
            "========== EPOCH 1 ==========\n",
            "Iteration 100 ==== Loss: 0.1286257952451706\n",
            "Iteration 200 ==== Loss: 0.5091476440429688\n",
            "Iteration 300 ==== Loss: 0.25829842686653137\n",
            "Iteration 400 ==== Loss: 0.11709516495466232\n",
            "Iteration 500 ==== Loss: 0.14344905316829681\n",
            "Iteration 600 ==== Loss: 0.09374682605266571\n",
            "Iteration 700 ==== Loss: 0.2250899374485016\n",
            "Time Taken for Epoch: 209.75870776176453\n",
            "\n",
            "========== Validating ==========\n",
            "Validation Loss: 0.4195776296507977\n",
            "Validation Accuracy: 0.8349184989929199\n",
            "\n",
            "\n",
            "========== EPOCH 2 ==========\n",
            "Iteration 100 ==== Loss: 0.005587403196841478\n",
            "Iteration 200 ==== Loss: 0.19125589728355408\n",
            "Iteration 300 ==== Loss: 0.008362794294953346\n",
            "Iteration 400 ==== Loss: 0.0024462032597512007\n",
            "Iteration 500 ==== Loss: 0.045290105044841766\n",
            "Iteration 600 ==== Loss: 0.002186747267842293\n",
            "Iteration 700 ==== Loss: 0.15362673997879028\n",
            "Time Taken for Epoch: 210.35472702980042\n",
            "\n",
            "========== Validating ==========\n",
            "Validation Loss: 0.7894485279872461\n",
            "Validation Accuracy: 0.8291639685630798\n",
            "\n",
            "\n",
            "========== EPOCH 3 ==========\n",
            "Iteration 100 ==== Loss: 0.000860357191413641\n",
            "Iteration 200 ==== Loss: 0.00528712710365653\n",
            "Iteration 300 ==== Loss: 0.0034393053501844406\n",
            "Iteration 400 ==== Loss: 0.0004279306740500033\n",
            "Iteration 500 ==== Loss: 0.043886516243219376\n",
            "Iteration 600 ==== Loss: 0.00044426246313378215\n",
            "Iteration 700 ==== Loss: 0.0007916594622656703\n",
            "Time Taken for Epoch: 207.81082844734192\n",
            "\n",
            "========== Validating ==========\n",
            "Validation Loss: 1.043787303352383\n",
            "Validation Accuracy: 0.8287643790245056\n",
            "CPU times: user 10min 58s, sys: 3min 34s, total: 14min 33s\n",
            "Wall time: 21min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GO7grF5FkWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving our model\n",
        "import os\n",
        "\n",
        "save_path = 'checkpoints'\n",
        "\n",
        "if not os.path.isdir(save_path):\n",
        "    os.mkdir(save_path)\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}, os.path.join(save_path,'model.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UcGA5SBb78J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "298cad0c-fa27-40a0-953d-6e9767af6e54"
      },
      "source": [
        "ls checkpoints/"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model.pth\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}